<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Avishree Khare | Cross Entropy method in Reinforcement Learning</title>
<meta name="description" content="My digital home: Info, Writings, and others.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2020/cross-entropy-method-rl/">

<!-- Google verification -->
<meta name="google-site-verification" content="5u4uolv2cd7ToSQ3yo5Fdw8iTnVAuWqGJSnTG43IyZY" />

<!-- Open Graph -->

<meta property="og:site_name" content="My digital home: Info, Writings, and others.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://avishreekh.github.io/blog/2020/cross-entropy-method-rl/" />
<meta property="og:description" content="Cross Entropy method in Reinforcement Learning" />
<meta property="og:image" content="/assets/img/9.jpg" />


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
        
        <a class="navbar-brand title font-weight-lighter" href="https://avishreekh.github.io/"><span class="font-weight-bold">Avishree</span> Khare</a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/books/">
                Books
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Cross Entropy method in Reinforcement Learning</h1>
    <p class="post-meta">July 4, 2020
      <span>
          
            
            <a class="post-tag" href="/tags/reinforcement-learning"><nobr>reinforcement-learning</nobr>&nbsp;</a>
          
            
            <a class="post-tag" href="/tags/cross-entropy-method"><nobr>cross-entropy-method</nobr>&nbsp;</a>
          
      </span>
    </p>
  </header>

  <article class="post-content">
    <p>If you have ever gathered the courage to explore the field of Reinforcement Learning, there are good chances that you found yourself lost in fancy vocabulary. Big words, names of complex algorithms with even more complex math behind them. But what if there were simpler, much more intuitive, and super-efficient algorithms out there that worked well?</p>

<p>Meet the Cross-Entropy Method: An evolutionary algorithm for parameterized policy optimization that John Schulman claims works “embarrassingly well” on complex RL problems².</p>

<hr />

<h5><b> What is Cross-Entropy Method? </b></h5>

<p>From a biological viewpoint, it is an Evolutionary Algorithm. Some individuals are sampled from a population and only the best ones govern the characteristics of future generations.</p>

<p>Mathematically, it can be seen as a Derivative-Free Optimization (DFO) technique, i.e., it can find optima without the overhead of calculating derivatives (no backpropagation!).</p>

<hr />

<h5><b> How does this method work? </b></h5>

<p>Assume for a second that you do not know what are agents, environments, and policies. You are just given a “black-box” which takes some numbers as inputs and outputs some other numbers. You can only choose the values for your inputs and observe the outputs. How do you guess the inputs such that the outputs are the values you want?</p>

<p>One simple way of doing this would be to take a bunch of inputs, see the outputs produced, choose the inputs that have led to the best outputs and tune them till you are satisfied with the outputs you see. This is essentially what the cross-entropy method does.</p>

<hr />

<h5><b> So, how do I use it to solve my RL problem? </b></h5>

<p>Let’s understand the working of CEM step-by-step with an example. I have added some python code snippets with each step for a better understanding of the implementation. The code is heavily borrowed from Udacity’s course on Deep Reinforcement Learning (amazing python RL resources btw, Github link at the end of this article)¹.</p>

<p>Consider your policy network. You want to find the best weights which can take the right “meaningful” actions based on your agent’s state. A CEM-based approach for finding these weights is as follows:</p>

<p><b>Step 1:</b> Draw a bunch of initial weights from a random distribution. Although this distribution is generally chosen to be Gaussian, you can choose any distribution that you believe the weights are from. Let’s say I drew 10 candidates for weights w1, w2, …, w10 from a Gaussian distribution with mean μ and variance σ².</p>

<p>Consider μ=0, σ=1, n_weights=10 (number of candidates) and weights_dim represents dimensions of the weight vector.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre> <span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span>       
 <span class="n">std</span> <span class="o">=</span> <span class="mf">1.0</span>
 <span class="n">n_weights</span> <span class="o">=</span> <span class="mi">10</span>
 <span class="n">weights_pop</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean</span> <span class="o">+</span> <span class="n">std</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">weights_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i_weight</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_weights</span><span class="p">)]</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><b>Step 2:</b> Now let the agent pick actions from the policy network based on these weights, run the agent through an episode and collect the rewards generated by the environment. For our example, say w1 generates a cumulative reward r1, w2 generates r2 and so on.</p>

<p>The evaluate method for an agent takes a weight candidate as input, plays an episode and outputs the cumulative reward from that episode.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre> <span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">agent</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="n">weights_pop</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><b>Step 3:</b> Find the weights which generated the best rewards. Assume the best 4 weights were w1, w2, w5 and w6 (also called the “elite” weights). Here 4 is a number that we have chosen. In general, you consider best n weights, where n is chosen by you.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre> <span class="n">n_elite</span> <span class="o">=</span> <span class="mi">4</span>
 <span class="n">elite_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">).</span><span class="n">argsort</span><span class="p">()[</span><span class="o">-</span><span class="n">n_elite</span><span class="p">:]</span>
 <span class="n">elite_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights_pop</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">elite_idxs</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><b>Step 4:</b> Pick the new weights from a distribution defined by the elite weights. Say μ’ is the mean of the best weights (w1, w2, w5 and w6) and σ’² is their variance. We now draw 10 candidates from a gaussian distribution with mean μ’ and variance σ’².</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre> <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">elite_weights</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
 <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">elite_weights</span><span class="p">).</span><span class="n">std</span><span class="p">()</span>
 <span class="n">weights_pop</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean</span> <span class="o">+</span> <span class="n">std</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">weights_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i_weight</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_weights</span><span class="p">)]</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p><b>Step 5:</b> Repeat steps 2–4 until you are happy with the rewards you get.</p>

<hr />

<p>If python code is not your thing and you love to read algorithms with math jargon, here is the pseudocode for you²:</p>

<div align="center" class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/cem_rl_pseudocode.png" />
    </div>
</div>
<div class="caption">
    Credits: MLSS 2016 on Deep Reinforcement Learning by John Schulman
</div>

<hr />

<h5><b>Conclusion</b></h5>

<p>Cross-Entropy Method is a simple algorithm that you can use for training RL agents. This method has outperformed several RL techniques on famous tasks including the game of Tetris⁴. You can use this as a baseline³ before moving to more complex RL algorithms like PPO, A3C, etc. There are several variants of CEM, however, the structure defined in this article is the backbone of all of them.</p>

<p>That concludes this article on the Cross-Entropy Method for Reinforcement Learning. I hope you liked what you just read and thank you for your time.</p>

<hr />

<h5><b>References</b></h5>

<p>[1] <a href="https://github.com/udacity/deep-reinforcement-learning/tree/master/cross-entropy">https://github.com/udacity/deep-reinforcement-learning/tree/master/cross-entropy</a></p>

<p>[2] <a href="https://www.youtube.com/watch?v=aUrX-rP_ss4">MLSS 2016 on Deep Reinforcement Learning by John Schulman</a></p>

<p>[3] <a href="http://karpathy.github.io/2016/05/31/rl/">http://karpathy.github.io/2016/05/31/rl/</a></p>

<p>[4] I. Szita and A. Lorincz, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Learning Tetris Using the Noisy Cross-Entropy Method</a> (2006), Neural Computation</p>

<hr />


  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'avishreekh-github-io';
      var disqus_identifier = '/blog/2020/cross-entropy-method-rl';
      var disqus_title      = "Cross Entropy method in Reinforcement Learning";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Avishree Khare.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>



<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {
  $('[data-toggle="tooltip"]').tooltip()
})
</script>



<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-160848892-1', 'auto');
ga('send', 'pageview');
</script>



</html>
